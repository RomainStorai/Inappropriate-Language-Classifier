{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Project - Inappropriate Language Classification\n",
    "\n",
    "Classification using 3 different models:\n",
    "- Decision tree model -> sourced from the sklearn.tree module. (This notebook)\n",
    "- Random forest model -> sourced from the sklearn.ensemble module. (This notebook)\n",
    "- LSTM model -> sourced from tanserflow.keras module. (LSTM notebook)\n",
    "\n",
    "There are two options for tockenisation and embedding:\n",
    "- **CountVectorizer** (tockenisation)\n",
    "- **GloVe model** (tockenisation + embedding)\n",
    "\n",
    "The **CountVectorizer** has it's dictionary created on the entire dataset, as such, it is limited to the words that are present in the dataset. This might contain words that haven't been seen by pretrained models. It is however entirely sure that pretrained models have seen certain words that aren't present in the dataset. Furethermore it doesn't embed the words, as such the models will need to infer their own relations between then words.\n",
    "\n",
    "The **GloVe model** is a pretrained model trained on a large corpus of words. It allows for both vectorization and embedding of the words. This allows for a first relation between different words to be created. For example, the embeddings of King - Man + Woman would equal Queen. Furthermore, it's dicitonary contains words that are not present in the dataset, this allows the entire trained model to be applied to be applied to new elements containing unseen words more effectively. For example, the word *biatch* not contained in the corpus would have it's embedding close to that of *bitch* which would allow the model to infer it's meaning.\n",
    "\n",
    "This Jupyter Notebook contains the following features:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "\n",
    "Here we will load the data, choose a tockeniser and preprocess the data by tockenising it. **Choose only one of the tockenisers**, rerunning a block will overwrite the other."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data - Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment_baseplate import get_split_count_vectorizer\n",
    "\n",
    "X_train, y_train, X_validate, y_validate, X_test, y_test = get_split_count_vectorizer()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data - GloVe\n",
    "\n",
    "There are three ways of using the glove model:\n",
    "1. Get the average of the vectors - works best\n",
    "2. Get the sum of the vectors - works ok\n",
    "3. Flatten all the vectors - requires pading, training with batches and doesn't work at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "If needed download weights\n",
    "'''\n",
    "from experiment_baseplate import get_glove_model\n",
    "\n",
    "get_glove_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment_baseplate import get_split_glove_embedding\n",
    "\n",
    "X_train, y_train, X_validate, y_validate, X_test, y_test = get_split_glove_embedding()\n",
    "\n",
    "y_processed=False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Get the average of the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def post_process_glove(X_values):\n",
    "    return np.array([np.mean(np.array(v), axis=0) for v in X_values])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Get the sum of the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def post_process_glove(X_values):\n",
    "    return np.array([np.sum(np.array(v), axis=0) for v in X_values])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Flatten all the vectors\n",
    "\n",
    "If you use this we recommand you use batches to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "max_input_length = 100\n",
    "\n",
    "def post_process_glove(X_values):\n",
    "    X_values = np.array( pad_sequences( X_values , maxlen=max_input_length) , np.uint8)\n",
    "    return X_values.reshape(-1, X_values.shape[1] * X_values.shape[2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = post_process_glove(X_train)\n",
    "X_validate = post_process_glove(X_validate)\n",
    "X_test = post_process_glove(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Decision Tree\n",
    "\n",
    "The decision tree model is built and trained by sklearn. Parameters are specified by the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "model = \"decision tree\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Random Forest\n",
    "\n",
    "The random forest model is built and trained by sklearn. Parameters are specified by the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "model = \"random forest\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Support Vector Machine\n",
    "\n",
    "The random forest model is built and trained by sklearn. Parameters are specified by the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "y_train = y_train[:, 1]\n",
    "y_validate = y_validate[:, 1]\n",
    "y_test = y_test[:, 1]\n",
    "y_processed=True\n",
    "\n",
    "clf = SVC()\n",
    "model = \"svc\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training\n",
    "\n",
    "Make sure you have imported the data, click on run, sit back and relax while the model trains."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training...\")\n",
    "clf.fit(X_train, y_train) \n",
    "print(\"Training finished...\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Batches\n",
    "\n",
    "If the data is too big to train in one go, do batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "print(\"Training...\")\n",
    "\n",
    "batch_size = 200\n",
    "iterations = math.floor(X_train.shape[0] / batch_size)\n",
    "for i in range(iterations):\n",
    "    clf.fit(X_train[(batch_size * i):(batch_size * (i+1))], y_train[(batch_size * i):(batch_size * (i+1))])\n",
    "if (iterations * batch_size < X_train.shape[0]):\n",
    "    clf.fit(X_train[(batch_size * iterations):], y_train[(batch_size * iterations):])\n",
    "    \n",
    "print(\"Training finished...\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing\n",
    "\n",
    "Once you have trained, click on run and get the results on unseen data. You will have both test results on the validate and test. That is because the validation dataset wasn't used during training and it is bigger. The testing dataset is good to compare with the other models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment_baseplate import score\n",
    "import time\n",
    "\n",
    "start = time.time_ns()\n",
    "X_val_predict = clf.predict(X_validate)\n",
    "val_time = time.time_ns() - start\n",
    "\n",
    "start = time.time_ns()\n",
    "X_test_predict = clf.predict(X_test)\n",
    "test_time = time.time_ns() - start\n",
    "\n",
    "print(model + \" Model\")\n",
    "print(f\"Validate values\\n\\t{score( X_val_predict , y_validate, y_processed=y_processed)} | inf_time : {val_time / X_validate.shape[0], y_processed=y_processed} ns\")\n",
    "print(f\"Test values\\n\\t{score( X_test_predict , y_test, y_processed=y_processed)} | inf_time : {test_time / X_test.shape[0], y_processed=y_processed} ns\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Saving\n",
    "\n",
    "Remeber to choose the proper save location!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_loc = 'data/your_model.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(file_loc, 'wb') as f:\n",
    "    pickle.dump(clf, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Loading\n",
    "\n",
    "Remeber to choose the proper load location!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_loc = 'data/your_model.pkl'\n",
    "model = \"your model name (for display purposes)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(file_loc, \"rb\") as f:\n",
    "    clf = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bad_lang_class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
